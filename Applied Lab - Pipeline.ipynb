{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "450cd6d5",
   "metadata": {},
   "source": [
    "# Applied Competitive Lab In Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199b6712",
   "metadata": {},
   "source": [
    "By: Micha Hashkes, Tamar Ben-Ami, Noa Bitan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d680e6",
   "metadata": {},
   "source": [
    "## Stage 1 - Reading DataFrame from fires DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fa8566b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.10.3-CAPI-1.16.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "# import rasterio\n",
    "import requests\n",
    "import sqlite3\n",
    "from shapely.wkt import loads\n",
    "from config import *\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21a77234",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_FIELD = \"STAT_CAUSE_CODE\"\n",
    "null_columns = ['ICS_209_INCIDENT_NUMBER', 'ICS_209_NAME', 'MTBS_ID', 'MTBS_FIRE_NAME', 'COMPLEX_NAME']\n",
    "leakadge_columns = ['FIRE_NAME', 'NWCG_REPORTING_AGENCY', 'NWCG_REPORTING_UNIT_ID', 'NWCG_REPORTING_UNIT_ID', 'SOURCE_REPORTING_UNIT_NAME',\n",
    "                       'ICS209NAME']\n",
    "id_columns = ['OBJECTID', 'FOD_ID', 'FPA_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84f29225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe():\n",
    "    conn = sqlite3.connect('FPA_FOD_20170508.sqlite')\n",
    "    query = \"\"\"\n",
    "    select a.*, \"POLYGON ((\" || b.xmin || \" \" || b.ymin || \", \" || b.xmax || \" \" || b.ymin || \", \" || b.xmax || \" \" || b.ymax || \", \" || b.xmin || \" \" || b.ymax || \", \" || b.xmin || \" \" || b.ymin || \"))\" BOX_GEOMETRY\n",
    "    from Fires a\n",
    "    join idx_Fires_Shape b\n",
    "    on a.OBJECTID = b.pkid\n",
    "    \"\"\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e89cc35",
   "metadata": {},
   "source": [
    "## Stage 2 - Utils Functions Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad561f7a",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def date_features(df):\n",
    "    df[\"cont_date_dt\"] = pd.to_datetime(df.CONT_DATE, origin=\"julian\",\n",
    "                                        unit='D')\n",
    "    df[\"disc_date_dt\"] = pd.to_datetime(df.DISCOVERY_DATE, origin=\"julian\",\n",
    "                                        unit='D')\n",
    "    df[\"cont_year\"] = df[\"cont_date_dt\"].apply(\n",
    "        lambda x: 0 if np.isnan(x.year) else int(x.year))\n",
    "    df[\"cont_mon\"] = df[\"cont_date_dt\"].apply(\n",
    "        lambda x: 0 if np.isnan(x.month) else int(x.month))\n",
    "    df[\"cont_dow\"] = df[\"cont_date_dt\"].apply(\n",
    "        lambda x: 0 if np.isnan(x.weekday()) else int(x.weekday()))\n",
    "    df[\"cont_is_weekend\"] = df[\"cont_dow\"].isin([6, 5, 4]).astype(int)\n",
    "    df[\"disc_year\"] = df[\"disc_date_dt\"].apply(\n",
    "        lambda x: 0 if np.isnan(x.year) else int(x.year))\n",
    "    df[\"disc_mon\"] = df[\"disc_date_dt\"].apply(\n",
    "        lambda x: 0 if np.isnan(x.month) else int(x.month))\n",
    "    df[\"disc_dow\"] = df[\"disc_date_dt\"].apply(\n",
    "        lambda x: 0 if np.isnan(x.weekday()) else int(x.weekday()))\n",
    "    df[\"disc_is_weekend\"] = df[\"disc_dow\"].isin([6, 5, 4]).astype(int)\n",
    "\n",
    "    df[\"time_to_cont\"] = df[\"cont_date_dt\"] - df[\"disc_date_dt\"]\n",
    "    df[\"time_to_cont\"] = df[\"time_to_cont\"].apply(\n",
    "        lambda x: 0 if pd.isnull(x) else x.days)\n",
    "\n",
    "    features = [\"cont_year\", \"cont_mon\", \"cont_dow\", \"cont_is_weekend\",\n",
    "                \"disc_year\", \"disc_mon\", \"disc_dow\", \"disc_is_weekend\",\n",
    "                \"time_to_cont\"]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b43ed9b4",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def aggregative_features(df):\n",
    "    # frequency per month\n",
    "    months_stats = df[\"DISCOVERY_MONTH\"].value_counts().reset_index().rename(columns={\"index\":\"DISCOVERY_MONTH\", \"DISCOVERY_MONTH\":\"MONTH_FREQ\"})\n",
    "    months_stats[\"MONTH_FREQ\"] = months_stats[\"MONTH_FREQ\"] / df.shape[0]\n",
    "    df = pd.merge(df, months_stats, left_on = [\"DISCOVERY_MONTH\"], right_on = [\"DISCOVERY_MONTH\"])\n",
    "    # frquency per day of week\n",
    "    weekday_stats = df[\"DISCOVERY_WEEKDAY\"].value_counts().reset_index().rename(columns={\"index\":\"DISCOVERY_WEEKDAY\", \"DISCOVERY_WEEKDAY\":\"WEEKDAY_FREQ\"})\n",
    "    weekday_stats[\"WEEKDAY_FREQ\"] = weekday_stats[\"WEEKDAY_FREQ\"] / df.shape[0]\n",
    "    df = pd.merge(df, weekday_stats, left_on = [\"DISCOVERY_WEEKDAY\"], right_on = [\"DISCOVERY_WEEKDAY\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59c98171",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def geo_vector_features(df, external_vector_gdfs, is_poly=False):\n",
    "    gdf = df_to_gdf(df)\n",
    "    features = []\n",
    "    for external_gdf, name_of_data, geo_oid_field in external_vector_gdfs:\n",
    "        gdf[f\"distances_{name_of_data}\"] = \\\n",
    "        gpd.sjoin_nearest(gdf, external_gdf, how='left', lsuffix='left',\n",
    "                          rsuffix='right',\n",
    "                          distance_col=f\"distances_{name_of_data}\")[\n",
    "            f\"distances_{name_of_data}\"]\n",
    "        features.append(f\"distances_{name_of_data}\")\n",
    "        df[f\"distances_{name_of_data}\"] = df[[OID_FIELD]].merge(gdf[[OID_FIELD, f\"distances_{name_of_data}\"]], left_on=OID_FIELD, right_on=OID_FIELD)[f\"distances_{name_of_data}\"]\n",
    "        # Gdf with polygons!\n",
    "        # if is_poly:\n",
    "        #     gs = gpd.sjoin(gdf, external_gdf, how='left', predicate=\"intersects\",\n",
    "        #               lsuffix='left', rsuffix='right').groupby(\n",
    "        #         OID_FIELD)[geo_oid_field].size().rename(f\"count_intersections_{name_of_data}\")\n",
    "        #     gdf = gdf.merge(gs, how=\"left\", left_on=OID_FIELD, right_index=True)\n",
    "        #     features.append(f\"count_intersections_{name_of_data}\")\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e906c0b",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def state_county_features_train(train_df):\n",
    "    features = [\"state_county_gb\", \"state_gb\"]\n",
    "    train_df[\"STATE_COUNTY\"] = train_df[\"STATE\"] + \"_\" + train_df[\"COUNTY\"]\n",
    "    gb_state_county = train_df.groupby(\"STATE_COUNTY\").size().rename(\n",
    "        \"state_county_gb\")\n",
    "    train_df[\"state_county_gb\"] = train_df.merge(gb_state_county, how=\"left\", right_index=True,\n",
    "                  left_on=\"STATE_COUNTY\")[\"state_county_gb\"]\n",
    "    gb_state = train_df.groupby(\"STATE\").size().rename(\"state_gb\")\n",
    "    train_df[\"state_gb\"] = train_df.merge(gb_state, how=\"left\", right_index=True,\n",
    "                  left_on=\"STATE\")[\"state_gb\"]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28221d34",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def state_county_features_test(test_df, train_df):\n",
    "    test_df[\"STATE_COUNTY\"] = test_df[\"STATE\"] + \"_\" + test_df[\"COUNTY\"]\n",
    "    test_df[\"state_county_gb\"] = test_df.merge(train_df[[\"state_county_gb\"]] , how=\"left\", right_index=True,\n",
    "                  left_on=\"STATE_COUNTY\")[\"state_county_gb\"]\n",
    "    test_df[\"state_gb\"] = test_df.merge(train_df[[\"state_gb\"]], how=\"left\", right_index=True,\n",
    "                  left_on=\"STATE\")[\"state_gb\"]\n",
    "    return [\"state_county_gb\", \"state_gb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b1f79b2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def df_to_gdf(df):\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df, geometry=gpd.points_from_xy(df.LONGITUDE, df.LATITUDE), crs=4269)\n",
    "    gdf = gdf.to_crs(4326)\n",
    "    gdf['LATITUDE'] = gdf.geometry.y\n",
    "    gdf['LONGITUDE'] = gdf.geometry.x\n",
    "    gdf['POINT_GEOMETRY'] = gdf.geometry\n",
    "    # gdf['BOX_GEOMETRY'] = gpd.GeoSeries(gdf['BOX_GEOMETRY'].apply(loads), crs=4269).to_crs(4326)\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1437291c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def fire_size_class_to_num(df):\n",
    "    df[\"FIRE_SIZE_CLASS\"] = df[\"FIRE_SIZE_CLASS\"].map({\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\" : 6, \"G\":7})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d365194",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_elevation(gdf):\n",
    "    url = \"https://api.opentopodata.org/v1/ned10m\"\n",
    "    elavation_dfs = []\n",
    "    all_coords_str = gdf.geometry.y.astype(str) + \",\" + gdf.geometry.x.astype(str)\n",
    "    for i in range(0, len(all_coords_str), 100):\n",
    "        coords = '|'.join(all_coords_str[i:i + 100])\n",
    "        data = {\n",
    "            \"locations\": coords,\n",
    "            \"interpolation\": \"cubic\"\n",
    "        }\n",
    "        response = requests.post(url, json=data)\n",
    "        elavation_dfs.append(pd.json_normalize(response.json(), \"results\"))\n",
    "    return pd.concat(elavation_dfs)['elevation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61f08295",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def weather_normal_features(gdf, data_path):\n",
    "    raster_files = {}\n",
    "    for feat, dir_name in WEATHER_FEATURES_MAP.items():\n",
    "        raster_files[feat] = {}\n",
    "        for i in range(1, 13):\n",
    "            month = str(i) if i >= 10 else f'0{i}'\n",
    "            bil_file = os.path.join(data_path, dir_name, WEATHER_FILES_MAP[dir_name].format(month))\n",
    "            raster_files[feat][month] = rasterio.open(bil_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be8c0c7c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def create_weather_features(row):\n",
    "    month_str = row['DISCOVERY_DATE'].strftime(\"%m\")\n",
    "    results = []\n",
    "    for feature in WEATHER_FEATURES_MAP:\n",
    "        data_value = list(raster_files[feature][month_str].sample([(row['LONGITUDE_NAD83'], row['LATITUDE_NAD83'])]))\n",
    "        results.append(data_value[0][0])\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "350bef6f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def print_feature_importance(rf_model):\n",
    "    importances = rf_model.feature_importances_\n",
    "    sorted_indices = np.argsort(importances)[::-1]\n",
    "    feat_labels = list(rf_model.feature_names_in_)\n",
    "    for f in range(len(feat_labels)):\n",
    "        print(\"%2d) %-*s %f\" % (f + 1, 30,\n",
    "                                feat_labels[sorted_indices[f]],\n",
    "                                importances[sorted_indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81131cc",
   "metadata": {},
   "source": [
    "## Stage 3 - Pipeline Functions Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b9c4eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_train(train_gdf):\n",
    "    \"\"\"function performs all basic data manipulations that are needed on both train and test dataframe\"\"\"\n",
    "    basic_features_lst = [\"LONGITUDE\", \"LATITUDE\", \"STATE\", \"COUNTY\", \"FIRE_SIZE\"]#, \"FIRE_SIZE_CLASS\"]\n",
    "    date_features_lst = date_features(train_gdf)\n",
    "    state_county_features = [] # state_county_features_train(train_gdf)\n",
    "    return basic_features_lst + date_features_lst + state_county_features\n",
    "\n",
    "def extract_features_test(test_gdf, train_gdf):\n",
    "    \"\"\"function performs all basic data manipulations that are needed on both train and test dataframe\"\"\"\n",
    "    basic_features_lst = [\"LONGITUDE\", \"LATITUDE\", \"STATE\", \"COUNTY\", \"FIRE_SIZE\"]#, \"FIRE_SIZE_CLASS\"]\n",
    "    date_features_lst = date_features(test_gdf)\n",
    "    state_county_features = [] # state_county_features_test(test_gdf, train_gdf)\n",
    "    return basic_features_lst + date_features_lst + state_county_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23bbd027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(X, y):\n",
    "    #to fill\n",
    "    # HyperParmaeter selection, etc\n",
    "    trained_model = None\n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d40ef671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_results(X_test, model):\n",
    "    pass\n",
    "    #return model.predict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37610f7",
   "metadata": {},
   "source": [
    "## Stage 4 - Pipeline Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a44e60a",
   "metadata": {},
   "source": [
    "#### In order to use our model, you'll need:\n",
    "1. put your to_test dataframe in to_test_df variable\n",
    "2. run \"run me\" cell which will create the trained model\n",
    "3. use \"predict_results\" function who takes test_df and a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750b532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run me\n",
    "# model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e075830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill me\n",
    "train_df = create_dataframe()\n",
    "train_df = train_df[100:]\n",
    "test_df = train_df[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a028025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gdf, test_gdf = df_to_gdf(train_df), df_to_gdf(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a350ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features = extract_features_train(train_gdf),\\\n",
    "                                extract_features_test(test_gdf, train_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23142236",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fit_model(train_gdf[train_features], train_gdf[LABEL_FIELD])\n",
    "test_results = predict_results(test_gdf[test_features], model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b382189",
   "metadata": {},
   "source": [
    "Noa:\n",
    "Cont time\n",
    "Exploratory Data Analysis\n",
    "Document \n",
    "\n",
    "Tamar: \n",
    "State County Features\n",
    "Model\n",
    "\n",
    "Micha:\n",
    "Geo Features + External Features\n",
    "Geo Graphs for EDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e432463b",
   "metadata": {},
   "source": [
    "# Open things for us\n",
    "1. should we split our data to train and test? or train on all fires db?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}