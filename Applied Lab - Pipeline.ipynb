{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "450cd6d5",
   "metadata": {},
   "source": [
    "# Applied Competitive Lab In Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199b6712",
   "metadata": {},
   "source": [
    "By: Micha Hashkes, Tamar Ben-Ami, Noa Bitan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d680e6",
   "metadata": {},
   "source": [
    "## Stage 1 - Reading DataFrame from fires DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fa8566b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16984/716902444.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# import rasterio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "# import rasterio\n",
    "import requests\n",
    "import sqlite3\n",
    "from shapely.wkt import loads\n",
    "from config import *\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a77234",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_FIELD = \"STAT_CAUSE_CODE\"\n",
    "null_columns = ['ICS_209_INCIDENT_NUMBER', 'ICS_209_NAME', 'MTBS_ID', 'MTBS_FIRE_NAME', 'COMPLEX_NAME']\n",
    "leakadge_columns = ['FIRE_NAME', 'NWCG_REPORTING_AGENCY', 'NWCG_REPORTING_UNIT_ID', 'NWCG_REPORTING_UNIT_ID', 'SOURCE_REPORTING_UNIT_NAME',\n",
    "                       'ICS209NAME']\n",
    "id_columns = ['OBJECTID', 'FOD_ID', 'FPA_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f29225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe():\n",
    "    conn = sqlite3.connect('FPA_FOD_20170508.sqlite')\n",
    "    query = \"\"\"\n",
    "    select a.*, \"POLYGON ((\" || b.xmin || \" \" || b.ymin || \", \" || b.xmax || \" \" || b.ymin || \", \" || b.xmax || \" \" || b.ymax || \", \" || b.xmin || \" \" || b.ymax || \", \" || b.xmin || \" \" || b.ymin || \"))\" BOX_GEOMETRY\n",
    "    from Fires a\n",
    "    join idx_Fires_Shape b\n",
    "    on a.OBJECTID = b.pkid\n",
    "    \"\"\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d46ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = createdataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e89cc35",
   "metadata": {},
   "source": [
    "## Stage 2 - Utils Functions Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad561f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_features(df):\n",
    "    df[\"cont_date_dt\"] = pd.to_datetime(df.CONT_DATE, origin=\"julian\",\n",
    "                                        unit='D')\n",
    "    df[\"disc_date_dt\"] = pd.to_datetime(df.DISCOVERY_DATE, origin=\"julian\",\n",
    "                                        unit='D')\n",
    "    df[\"cont_year\"] = df[\"cont_date_dt\"].apply(\n",
    "        lambda x: 0 if np.isnan(x.year) else int(x.year))\n",
    "    df[\"cont_mon\"] = df[\"cont_date_dt\"].apply(\n",
    "        lambda x: 0 if np.isnan(x.month) else int(x.month))\n",
    "    df[\"cont_dow\"] = df[\"cont_date_dt\"].apply(\n",
    "        lambda x: 0 if np.isnan(x.weekday()) else int(x.weekday()))\n",
    "    df[\"cont_is_weekend\"] = df[\"cont_dow\"].isin([6, 5, 4]).astype(int)\n",
    "    df[\"disc_year\"] = df[\"disc_date_dt\"].apply(\n",
    "        lambda x: 0 if np.isnan(x.year) else int(x.year))\n",
    "    df[\"disc_mon\"] = df[\"disc_date_dt\"].apply(\n",
    "        lambda x: 0 if np.isnan(x.month) else int(x.month))\n",
    "    df[\"disc_dow\"] = df[\"disc_date_dt\"].apply(\n",
    "        lambda x: 0 if np.isnan(x.weekday()) else int(x.weekday()))\n",
    "    df[\"disc_is_weekend\"] = df[\"disc_dow\"].isin([6, 5, 4]).astype(int)\n",
    "\n",
    "    df[\"time_to_cont\"] = df[\"cont_date_dt\"] - df[\"disc_date_dt\"]\n",
    "    df[\"time_to_cont\"] = df[\"time_to_cont\"].apply(\n",
    "        lambda x: 0 if pd.isnull(x) else x.days)\n",
    "\n",
    "    features = [\"cont_year\", \"cont_mon\", \"cont_dow\", \"cont_is_weekend\",\n",
    "                \"disc_year\", \"disc_mon\", \"disc_dow\", \"disc_is_weekend\",\n",
    "                \"time_to_cont\"]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43ed9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregative_features(df):\n",
    "    # frequency per month\n",
    "    months_stats = df[\"DISCOVERY_MONTH\"].value_counts().reset_index().rename(columns={\"index\":\"DISCOVERY_MONTH\", \"DISCOVERY_MONTH\":\"MONTH_FREQ\"})\n",
    "    months_stats[\"MONTH_FREQ\"] = months_stats[\"MONTH_FREQ\"] / df.shape[0]\n",
    "    df = pd.merge(df, months_stats, left_on = [\"DISCOVERY_MONTH\"], right_on = [\"DISCOVERY_MONTH\"])\n",
    "    # frquency per day of week\n",
    "    weekday_stats = df[\"DISCOVERY_WEEKDAY\"].value_counts().reset_index().rename(columns={\"index\":\"DISCOVERY_WEEKDAY\", \"DISCOVERY_WEEKDAY\":\"WEEKDAY_FREQ\"})\n",
    "    weekday_stats[\"WEEKDAY_FREQ\"] = weekday_stats[\"WEEKDAY_FREQ\"] / df.shape[0]\n",
    "    df = pd.merge(df, weekday_stats, left_on = [\"DISCOVERY_WEEKDAY\"], right_on = [\"DISCOVERY_WEEKDAY\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c98171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_vector_features(df, external_vector_gdfs, is_poly=False):\n",
    "    gdf = df_to_gdf(df)\n",
    "    features = []\n",
    "    for external_gdf, name_of_data, geo_oid_field in external_vector_gdfs:\n",
    "        gdf[f\"distances_{name_of_data}\"] = \\\n",
    "        gpd.sjoin_nearest(gdf, external_gdf, how='left', lsuffix='left',\n",
    "                          rsuffix='right',\n",
    "                          distance_col=f\"distances_{name_of_data}\")[\n",
    "            f\"distances_{name_of_data}\"]\n",
    "        features.append(f\"distances_{name_of_data}\")\n",
    "        df[f\"distances_{name_of_data}\"] = df[[OID_FIELD]].merge(gdf[[OID_FIELD, f\"distances_{name_of_data}\"]], left_on=OID_FIELD, right_on=OID_FIELD)[f\"distances_{name_of_data}\"]\n",
    "        # Gdf with polygons!\n",
    "        # if is_poly:\n",
    "        #     gs = gpd.sjoin(gdf, external_gdf, how='left', predicate=\"intersects\",\n",
    "        #               lsuffix='left', rsuffix='right').groupby(\n",
    "        #         OID_FIELD)[geo_oid_field].size().rename(f\"count_intersections_{name_of_data}\")\n",
    "        #     gdf = gdf.merge(gs, how=\"left\", left_on=OID_FIELD, right_index=True)\n",
    "        #     features.append(f\"count_intersections_{name_of_data}\")\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e906c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_county_features_train(train_df):\n",
    "    features = [\"state_county_gb\", \"state_gb\"]\n",
    "    train_df[\"STATE_COUNTY\"] = train_df[\"STATE\"] + \"_\" + train_df[\"COUNTY\"]\n",
    "    gb_state_county = train_df.groupby(\"STATE_COUNTY\").size().rename(\n",
    "        \"state_county_gb\")\n",
    "    train_df[\"state_county_gb\"] = train_df.merge(gb_state_county, how=\"left\", right_index=True,\n",
    "                  left_on=\"STATE_COUNTY\")[\"state_county_gb\"]\n",
    "    gb_state = train_df.groupby(\"STATE\").size().rename(\"state_gb\")\n",
    "    train_df[\"state_gb\"] = train_df.merge(gb_state, how=\"left\", right_index=True,\n",
    "                  left_on=\"STATE\")[\"state_gb\"]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28221d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_county_features_test(test_df, train_df):\n",
    "    test_df[\"STATE_COUNTY\"] = test_df[\"STATE\"] + \"_\" + test_df[\"COUNTY\"]\n",
    "    test_df[\"state_county_gb\"] = test_df.merge(train_df[[\"state_county_gb\"]] , how=\"left\", right_index=True,\n",
    "                  left_on=\"STATE_COUNTY\")[\"state_county_gb\"]\n",
    "    test_df[\"state_gb\"] = test_df.merge(train_df[[\"state_gb\"]], how=\"left\", right_index=True,\n",
    "                  left_on=\"STATE\")[\"state_gb\"]\n",
    "    return [\"state_county_gb\", \"state_gb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1f79b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_gdf(df):\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df, geometry=gpd.points_from_xy(df.LONGITUDE, df.LATITUDE), crs=4269)\n",
    "    gdf = gdf.to_crs(4326)\n",
    "    gdf['LATITUDE'] = gdf.geometry.y\n",
    "    gdf['LONGITUDE'] = gdf.geometry.x\n",
    "    gdf['POINT_GEOMETRY'] = gdf.geometry\n",
    "    # gdf['BOX_GEOMETRY'] = gpd.GeoSeries(gdf['BOX_GEOMETRY'].apply(loads), crs=4269).to_crs(4326)\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1437291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fire_size_class_to_num(df):\n",
    "    df[\"FIRE_SIZE_CLASS\"] = df[\"FIRE_SIZE_CLASS\"].map({\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\" : 6, \"G\":7})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d365194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elevation(gdf):\n",
    "    url = \"https://api.opentopodata.org/v1/ned10m\"\n",
    "    elavation_dfs = []\n",
    "    all_coords_str = gdf.geometry.y.astype(str) + \",\" + gdf.geometry.x.astype(str)\n",
    "    for i in range(0, len(all_coords_str), 100):\n",
    "        coords = '|'.join(all_coords_str[i:i + 100])\n",
    "        data = {\n",
    "            \"locations\": coords,\n",
    "            \"interpolation\": \"cubic\"\n",
    "        }\n",
    "        response = requests.post(url, json=data)\n",
    "        elavation_dfs.append(pd.json_normalize(response.json(), \"results\"))\n",
    "    return pd.concat(elavation_dfs)['elevation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f08295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_normal_features(gdf, data_path):\n",
    "    raster_files = {}\n",
    "    for feat, dir_name in WEATHER_FEATURES_MAP.items():\n",
    "        raster_files[feat] = {}\n",
    "        for i in range(1, 13):\n",
    "            month = str(i) if i >= 10 else f'0{i}'\n",
    "            bil_file = os.path.join(data_path, dir_name, WEATHER_FILES_MAP[dir_name].format(month))\n",
    "            raster_files[feat][month] = rasterio.open(bil_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8c0c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weather_features(row):\n",
    "    month_str = row['DISCOVERY_DATE'].strftime(\"%m\")\n",
    "    results = []\n",
    "    for feature in WEATHER_FEATURES_MAP:\n",
    "        data_value = list(raster_files[feature][month_str].sample([(row['LONGITUDE_NAD83'], row['LATITUDE_NAD83'])]))\n",
    "        results.append(data_value[0][0])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e85d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df, train_df=None):\n",
    "    basic_features_lst = [\"LONGITUDE\", \"LATITUDE\", \"STATE\", \"COUNTY\"]\n",
    "    date_features_lst = date_features(df)\n",
    "    if train_df:\n",
    "        state_county_features = state_county_features_test(df, train_df)\n",
    "    else:\n",
    "        state_county_features = state_county_features_train(df)\n",
    "    return basic_features_lst + date_features_lst + state_county_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350bef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_feature_importance(rf_model):\n",
    "    importances = rf_model.feature_importances_\n",
    "    sorted_indices = np.argsort(importances)[::-1]\n",
    "    feat_labels = list(rf_model.feature_names_in_)\n",
    "    for f in range(len(feat_labels)):\n",
    "        print(\"%2d) %-*s %f\" % (f + 1, 30,\n",
    "                                feat_labels[sorted_indices[f]],\n",
    "                                importances[sorted_indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81131cc",
   "metadata": {},
   "source": [
    "## Stage 3 - Pipeline Functions Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9c4eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_full_df(df, is_test=False):\n",
    "    \"\"\"function performs all basic data manipulations that are needed on both train and test dataframe\"\"\"\n",
    "    basic_features_lst = [\"LONGITUDE\", \"LATITUDE\", \"STATE\", \"COUNTY\", \"FIRE_SIZE\", \"FIRE_SIZE_CLASS\"]\n",
    "    date_features_lst = date_features(df)\n",
    "    if is_test:\n",
    "        df = df[date_features_lst + basic_features_lst]\n",
    "    else:\n",
    "        df = df[date_features_lst + basic_features_lst + [LABEL_FIELD]]\n",
    "    df = fire_size_class_to_num(df)\n",
    "    return aggregative_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29bc865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_train_df(df):\n",
    "    \"\"\"function performs all needed pre-processing actions on original dataframe, and return a processed dataframe for training\"\"\"\n",
    "    # to fill\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613a8211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_test_df(df):\n",
    "    # to fill\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bbd027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(X_train, y_train):\n",
    "    #to fill\n",
    "    trained_model = None\n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40ef671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_results(test_df, model):\n",
    "    #return model.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6786674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    org_df = create_dataframe()\n",
    "    df = pre_process_full_df(org_df)\n",
    "    X, y = df.drop(columns=[LABEL_FIELD]), df[LABEL_FIELD]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "    \n",
    "    X_train = pre_process_train_df(X_train)\n",
    "    X_test = pre_process_test_df(X_test)\n",
    "    \n",
    "    model = fit_model(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37610f7",
   "metadata": {},
   "source": [
    "## Stage 4 - Pipeline Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a44e60a",
   "metadata": {},
   "source": [
    "#### In order to use our model, you'll need:\n",
    "1. put your to_test dataframe in to_test_df variable\n",
    "2. run \"run me\" cell which will create the trained model\n",
    "3. use \"predict_results\" function who takes test_df and a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750b532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run me\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5078d500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill me\n",
    "to_test_df = None\n",
    "to_test_df = pre_process_full_df(to_test_df, True)\n",
    "to_test_df = pre_process_test_df(to_test_df)\n",
    "test_results = predict_results(to_test_df, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e432463b",
   "metadata": {},
   "source": [
    "# Open things for us\n",
    "1. should we split our data to train and test? or train on all fires db?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
